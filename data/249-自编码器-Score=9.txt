自编码器（autoencoder, AE）是一类在半监督学习和非监督学习中使用的人工神经网络（Artificial Neural Networks, ANNs），其功能是通过将输入信息作为学习目标，对输入信息进行表征学习（representation learning）。
自编码器包含编码器（encoder）和解码器（decoder）两部分。按学习范式，自编码器可以被分为收缩自编码器（contractive autoencoder）、正则自编码器（regularized autoencoder）和变分自编码器（Variational AutoEncoder, VAE），其中前两者是判别模型、后者是生成模型。按构筑类型，自编码器可以是前馈结构或递归结构的神经网络。
自编码器具有一般意义上表征学习算法的功能，被应用于降维（dimensionality reduction）和异常值检测（anomaly detection）。包含卷积层构筑的自编码器可被应用于计算机视觉问题，包括图像降噪（image denoising）、神经风格迁移（neural style transfer）等。

## 发展历史
自编码器在其研究早期是为解决表征学习中的“编码器问题（encoder problem）”，即基于神经网络的降维问题而提出的联结主义模型的学习算法。1985年，David H. Ackley、Geoffrey E. Hinton和Terrence J. Sejnowski在玻尔兹曼机上对自编码器算法进行了首次尝试，并通过模型权重对其表征学习能力进行了讨论。在1986年反向传播算法（Back-Propagation, BP）被正式提出后，自编码器算法作为BP的实现之一，即“自监督的反向传播（Self-supervised BP）”得到了研究，并在1987年被Jeffrey L. Elman和David Zipser用于语音数据的表征学习试验。自编码器作为一类神经网络结构（包含编码器和解码器两部分）的正式提出，来自1987年Yann LeCun发表的研究。LeCun (1987)使用多层感知器（Multi-Layer Perceptron,MLP）构建了包含编码器和解码器的神经网络，并将其用于数据降噪。此外，在同一时期，Bourlard and Kamp (1988)使用MLP自编码器对数据降维进行的研究也得到了关注。1994年，Hinton和Richard S. Zemel通过提出“最小描述长度原理（Minimum Description Length principle, MDL）”构建了第一个基于自编码器的生成模型。

## 算法
自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。给定输入空间和特征空间，自编码器求解两者的映射使输入特征的重建误差达成最小：
求解完成后，由编码器输出的隐含层特征，即“编码特征（encoded feature）”可视为输入数据的表征。按自编码器的不同，其编码特征可以是输入数据的压缩（收缩自编码器）、稀疏化（稀疏自编码器）或隐变量模型（变分自编码器）等。

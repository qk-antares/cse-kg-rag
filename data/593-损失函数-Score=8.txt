损失函数（loss function）或代价函数（cost function）是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。例如在统计学和机器学习中被用于模型的参数估计（parametric estimation），在宏观经济学中被用于风险管理（risk management）和决策，在控制理论中被应用于最优控制理论（optimal control theory）。

## 函数定义
在样本空间内有可测状态和随机变量根据法则所做的决策，此时若在乘积空间上有函数满足：，即对任意的，是非负可测函数，则被称为损失函数，表示状态下采取决策所对应的损失或风险。
机器学习中，给定独立同分布（independent and identically distributed,iid）的学习样本，和模型，损失函数是模型输出和观测结果间概率分布差异的量化：
式中表示模型参数，上式右侧具体的量化方法视问题和模型而定，但要求满足损失函数的一般定义，即样本空间的非负可测函数。

## 函数分类

### 回归问题
回归问题所对应的损失函数为L2损失函数和L1损失函数，二者度量了模型估计值与观测值之间的差异：
式中为真实值的权重，为真实值，为模型的输出。各类回归模型，例如线性回归、广义线性模型（Generalized Linear Model, GLM）和人工神经网络（Artificial Neural Network, ANN）通过最小化L2或L1损失对其参数进行估计。L2损失和L1损失的不同在于，L2损失通过平方计算放大了估计值和真实值的距离，因此对偏离观测值的输出给予很大的惩罚。此外，L2损失是平滑函数，在求解其优化问题时有利于误差梯度的计算；L1损失对估计值和真实值之差取绝对值，对偏离真实值的输出不敏感，因此在观测中存在异常值时有利于保持模型稳定。

### 分类问题
分类问题所对应的损失函数为0-1损失，其是分类准确度的度量，对分类正确的估计值取0，反之取1：
0-1损失函数是一个不连续的分段函数，不利于求解其最小化问题，因此在应用可构造其代理损失（surrogate loss）。代理损失是与原损失函数具有相合性（consistency）的损失函数，最小化代理损失所得的模型参数也是最小化原损失函数的解。当一个函数是连续凸函数，并在任意取值下是0-1损失函数的上界时，该函数可作为0-1损失函数的代理函数。
这里给出二元分类（binary classification）中0-1损失函数的代理损失：
名称
表达式
铰链损失函数（hinge loss function）

交叉熵损失函数（cross-entropy loss function）

指数损失函数（exponential loss function）

铰链损失（实线）、交叉熵损失（点）、指数损失（虚线）铰链损失（实线）、交叉熵损失（点）、指数损失（虚线）
铰链损失函数是一个分段连续函数，其在分类器分类完全正确时取0。使用铰链损失对应的分类器是支持向量机（Support Vector Machine, SVM），铰链损失的性质决定了SVM具有稀疏性，即分类正确但概率不足1和分类错误的样本被识别为支持向量（support vector）被用于划分决策边界，其余分类完全正确的样本没有参与模型求解。
交叉熵损失函数是一个平滑函数，其本质是信息理论（information theory）中的交叉熵（cross entropy）在分类问题中的应用。由交叉熵的定义可知，最小化交叉熵等价于最小化观测值和估计值的相对熵（relative entropy），即两者概率分布的Kullback-Leibler散度：，因此其是一个提供无偏估计的代理损失。交叉熵损失函数是表中使用最广泛的代理损失，对应的分类器例子包括logistic回归、人工神经网络和概率输出的支持向量机。
指数损失函数是表中对错误分类施加最大惩罚的损失函数，因此其优势是误差梯度大，对应的极小值问题在使用梯度算法时求解速度快。使用指数损失的分类器通常为自适应提升算法（Adaptive Boosting, AdaBoost），AdaBoot利用指数损失易于计算的特点，构建多个可快速求解的“弱”分类器成员并按成员表现进行赋权和迭代，组合得到一个“强”分类器并输出结果。

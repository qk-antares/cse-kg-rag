注意力机制（Attention Mechanism）源于对人类视觉的研究。在认知科学中，由于信息处理的瓶颈，人类会选择性地关注所有信息的一部分，同时忽略其他可见的信息。上述机制通常被称为注意力机制。人类视网膜不同的部位具有不同程度的信息处理能力，即敏锐度（Acuity），只有视网膜中央凹部位具有最强的敏锐度。为了合理利用有限的视觉信息处理资源，人类需要选择视觉区域中的特定部分，然后集中关注它。例如，人们在阅读时，通常只有少量要被读取的词会被关注和处理。综上，注意力机制主要有两个方面：决定需要关注输入的哪部分；分配有限的信息处理资源给重要的部分。

## 简介
注意力机制的一种非正式的说法是，神经注意力机制可以使得神经网络具备专注于其输入（或特征）子集的能力：选择特定的输入。注意力可以应用于任何类型的输入而不管其形状如何。在计算能力有限情况下，注意力机制（attention mechanism）是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。
注意力一般分为两种：一种是自上而下的有意识的注意力，称为聚焦式（focus）注意力。聚焦式注意力是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；另一种是自下而上的无意识的注意力，称为基于显著性（saliency-based）的注意力。基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关。如果一个对象的刺激信息不同于其周围信息，一种无意识的“赢者通吃”（winner-take-all）或者门控（gating）机制就可以把注意力转向这个对象。不管这些注意力是有意还是无意，大部分的人脑活动都需要依赖注意力，比如记忆信息，阅读或思考等。
在认知神经学中，注意力是一种人类不可或缺的复杂认知功能，指人可以在关注一些信息的同时忽略另一些信息的选择能力。在日常生活中，我们通过视觉、听觉、触觉等方式接收大量的感觉输入。但是我们的人脑可以在这些外界的信息轰炸中还能有条不紊地工作，是因为人脑可以有意或无意地从这些大量输入信息中选择小部分的有用信息来重点处理，并忽略其他信息。这种能力就叫做注意力。注意力可以体现为外部的刺激（听觉、视觉、味觉等），也可以体现为内部的意识（思考、回忆等）。

## 注意力机制的变体
多头注意力（multi-head attention）是利用多个查询，来平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。 硬注意力，即基于注意力分布的所有输入信息的期望。还有一种注意力是只关注到一个位置上，叫做硬性注意力（hardattention）。
硬性注意力有两种实现方式，一种是选取最高概率的输入信息。另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。
键值对注意力：更一般地，我们可以用键值对（key-value pair）格式来表示输入信息，其中“键”用来计算注意力分布，“值”用来生成选择的信息。
结构化注意力：要从输入信息中选取出和任务相关的信息，主动注意力是在所有输入信息上的多项分布，是一种扁平（flat）结构。如果输入信息本身具有层次（hierarchical）结构，比如文本可以分为词、句子、段落、篇章等不同粒度的层次，我们可以使用层次化的注意力来进行更好的信息选择[Yang et al., 2016]。此外，还可以假设注意力上下文相关的二项分布，用一种图模型来构建更复杂的结构化注意力分布。

## 应用
神经机器翻译
注意力机制最成功的应用是机器翻译。基于神经网络的机器翻译模型也叫做神经机器翻译（Neural Machine Translation，NMT）。一般的神经机器翻译模型采用“编码-解码”的方式进行序列到序列的转换。这种方式有两个问题：一是编码向量的容量瓶颈问题，即源语言所有的信息都需要保存在编码向量中，才能进行有效地解码；二是长距离依赖问题，即编码和解码过程中在长距离信息传递中的信息丢失问题。通过引入注意力机制，我们将源语言中每个位置的信息都保存下来。在解码过程中生成每一个目标语言的单词时，我们都通过注意力机制直接从源语言的信息中选择相关的信息作为辅助。这样的方式就可以有效地解决上面的两个问题。一是无需让所有的源语言信息都通过编码向量进行传递，在解码的每一步都可以直接访问源语言的所有位置上的信息；二是源语言的信息可以直接传递到解码过程中的每一步，缩短了信息传递的距离。
图像描述生成
图像描述生成是输入一幅图像，输出这幅图像对应的描述。图像描述生成也是采用“编码-解码”的方式进行。编码器为一个卷积网络，提取图像的高层特征，表示为一个编码向量；解码器为一个循环神经网络语言模型，初始输入为编码向量，生成图像的描述文本。在图像描述生成的任务中，同样存在编码容量瓶颈以及长距离依赖这两个问题，因此也可以利用注意力机制来有效地选择信息。在生成描述的每一个单词时，循环神经网络的输入除了前一个词的信息，还有利用注意力机制来选择一些来自于图像的相关信息。

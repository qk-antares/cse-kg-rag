反向传播算法，简称BP算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。

## 动机
反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。反向传播避免了重复子表达式的指数爆炸。然而，其他算法可能通过对计算图进行简化来避免更多的子表达式，或者也可能通过重新计算而不是存储这些子表达式来节省内存。

## 算法简介
BP网络的结构BP网络的结构BP算法(即误差反向传播算法)适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。
反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。
BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。

### 激励传播
每次迭代中的传播环节包含两步：
(前向传播阶段)将训练输入送入网络以获得激励响应；
(反向传播阶段)将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。

### 权重更新
对于每个突触上的权重，按照以下步骤进行更新：
将输入激励和响应误差相乘，从而获得权重的梯度；
将这个梯度乘上一个比例并取反后加到权重上。
这个比例将会影响到训练过程的速度和效果，因此称为“训练因子”。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。

## 产生和发展

### 感知器
1958年，心理学家Rosenblatt提出了最早的前馈层次网络摸型，并称为感知器(Perceptroa)。在这种模型中．输入图形，通过各输入结点分配给下一层的各结点，这下一层就是所谓中间层，中间层可以是一层也可以是多层，最后通过输出层结点得到输出图形。在这类前馈网络既没有反馈连接，没有层内连接，也没有隔层的前馈连接，每一结点只能前馈连到其下一层的所有结点。然而，对于含有隐蔽层的多层感知器当时没有可行的训练办法，所以初期研究的感知器为一层感知器。1969年，Minskey和Papert对Rosenblatt提出的简单感知器盛行了详细的分析。他们引用的一个典型侧子是所谓XOR(exclusive—or)问题Minskey和Papert指出没有隐层的简单感知器在许多像XOR问题的情形下显得无能为力，并证明了简单感知器只能解决线性分类问题和一阶谓诃同题。对于非线性分类问题和高阶谓词问题，必须引用隐单元层。隐单元可以在某一权值下对输入模式进行再编码，使得在新编码中模式的相似性能支持任何需要的输入输出映射，而不再像简单感知器那样使映射难以实现。

### BP算法
隐层的引入使网络具有很大的潜力。但正像Minskey和Papert当时所指出的．虽然对所有那些能用简单(无隐层)网结解决的问题有非常简单的学习规则，即简单感知器的收敛程序(主要归功于Widrow和HMf于1960年提出的Delta规刚)，
BP算法BP算法但当时并没有找到同样有效的含隐层的网络的学习规则。对此问题的研究有三个基本的结果。一种是使用简单无监督学习规则的竞争学习方法．但它缺乏外部信息．难以确定适合映射的隐层结构。第二条途径是假设一个内部(隐层)的表示方法，这在一些先约条件下是合理的。另一种方法是利用统计手段设计一个学习过程使之能有效地实现适当的内部表示法，Hinton等人(1984年)提出的Bolzmann机是这种方法的典型例子．它要求网络在两个不同的状态下达到平衡，并且只局限于对称网络。Barto和他的同事(1985年)提出了另一条利用统计手段的学习方法。但迄今为止最有效和最实用的方法是Rumelhart、Hinton和Williams(1986年)提出的一般Delta法则，即反向传播(BP)算法。Parter(1985年)也独立地得出过相似的算法,他称之为学习逻辑。此外， Lecun(1985年)也研究出大致相似的学习法则。

## BP网络特性

### 实现映射能力
神经元模型及其学习过程神经元模型及其学习过程BP网络的学习过程是一种误差修正型学习算法，由正向传播和反向传播组成。在正向传播过程中，输入信号从输入层通过作用函数后．逐层向隐含层，输出层传播，每一层神经元状态只影响下一层神经元状态。如果在输出层得不到期望的输出，则转入反向传播，将链接信号沿原来的连接通路返回。通过修改各层神经元的连接权值．使得输出误差信号最小 BP网络的连接结构和映射过程与多层Perceptmn相同，只是后者阶跃的单元激发函数被换作Sigmoid函数.Rumelhart等人在1985年重新发现用梯度法修改权重(generalized delta rule)的样本学习算法可以有效地运用在多层网络上，使得过去在Pereeptron模型中无能为力的XOR等学习问题获得解决。含有输入、输出和单层隐单元的三层BP网络富有的功能引起人们的注意。Lippmann（1987年）模式识别问题。
Wielanfl和Lelghton(1987年)给出了一个例子，用三层网络将空间划分成凹的子空间。Huang和Lipmann(1987年)仿真演示了三层网络可以处理几种很复杂的摸式辩识问题，这些研究促进了三层网络的广泛应用。Funashi和Hecht—Nielsen （1989年）分别证明了随着隐单元的增加，三层网络所实现的映射可以一致逼近紧急上的连续函数或按L范数逼近紧集上平方可积的函数．揭示了三层网络丰富的实现映射能力。

### 记忆机制
Mitchison和Durbin(1989年)给出在一定条件下，三层网络学习容量的上、下限的估计。三层网络的转入和输出单元都由应用的问题所规定，只有隐单元的数且是可变的 应行仁(1990年)详细分析三层神经网络的记忆机制，指出具有足够多隐单元的三层神经网络可以记忆任给的样本集。采用渐进函数(非常一般的函数．包括阶跃函数、Sigmoid函数等)作为隐单元激发匾数的三层神经网络。k-1个隐单元能够准确记忆k个实验值样本。采用阶跃激发函数时．k+1个随机给定的实数值样本能够被k+1个隐单元的阿络记忆的概率为零。联想记忆在Signaoid激发函数的网络中结果也是如此。

### 容错性
BP网络除具有较强的对信息分布式记忆特点外，还具有一定的容错性和抗干扰性．孙德保、高超对三层BP网络的容错性和抗干扰性进行了研究，得出了三层BP网络的容错能力取决于输入层到隐含层的连接权值矩阵与隐含层到辖出层连接权值矩阵的乘积的结果。
BP网络可以较好地实现宽频带、小噪比、信号模式较少情况下的信号识别和信噪分离。

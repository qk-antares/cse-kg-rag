监督学习是利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。
监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成。

## 定义
监督学习，也被称为监督机器学习，是机器学习和人工智能的一个子类。它的定义是使用标记数据集来训练算法，以便对数据进行分类或准确预测结果。当输入数据被输入到模型中时，它会调整其权重，直到模型被适当地拟合，这是交叉验证过程的一部分。监督式学习可以帮助组织大规模地解决各种现实世界的问题，例如对收件箱中单独文件夹中的垃圾邮件进行分类。

## 原理
监督学习使用一个训练集来教模型产生期望的输出。这个训练数据集包括输入和正确的输出，这使得模型可以随着时间的推移而学习。该算法通过损失函数测量其精度，调整直到误差被充分最小化。
当数据挖掘-分类和回归时，监督学习可以分为两类问题:
分类使用一种算法将测试数据准确地分配到特定的类别中。它识别数据集中的特定实体，并试图得出关于如何标记或定义这些实体的一些结论。常见的分类算法有线性分类器、支持向量机(SVM)、决策树、k近邻和随机森林，下面将对它们进行更详细的描述。
回归是用来理解因变量和自变量之间的关系。它通常用于预测，例如预测给定业务的销售收入。线性回归、逻辑回归和多项式回归是常用的回归算法。

## 特点
1、偏置方差权衡
第一个问题就是偏置和方差之间的权衡。假设有几种不同的,但同样好的演算数据集。一种学习算法是基于一个未知数的输入，在经过这些数据集的计算时,系统会无误的预测到并将正确的未知数输出。一个学习算法在不同的演算集演算时如果预测到不同的输出值会对特定的输入有较高的方差。一个预测误差学习分类器是与学习算法中的偏差和方差有关的。一般来说,偏差和方差之间有一个权衡。较低的学习算法偏差必须“灵活”,这样就可以很好的匹配数据。但如果学习算法过于灵活,它将匹配每个不同的训练数据集,因此有很高的方差。许多监督学习方法的一个关键方面是他们能够调整这个偏差和方差之间的权衡(通过提供一个偏见/方差参数,用户可以调整)。
2、功能的复杂性和数量的训练数据
第二个问题是训练数据可相对于“真正的”功能（分类或回归函数）的复杂度的量。如果真正的功能是简单的，则一个“不灵活的”学习算法具有高偏压和低的方差将能够从一个小数据量的学习。但是，如果真功能是非常复杂的（例如，因为它涉及在许多不同的输入要素的复杂的相互作用，并且行为与在输入空间的不同部分），则该函数将只从一个非常大的数量的训练数据，并使用可学习“灵活”的学习算法具有低偏置和高方差。因此，良好的学习算法来自动调整的基础上可用的数据量和该函数的明显的复杂性要学习的偏压/方差权衡。
3、输入空间的维数
第三个问题是输入空间的维数。如果输入特征向量具有非常高的维数，学习问题是很困难的，即使真函数仅依赖于一个小数目的那些特征。这是因为许多“额外”的尺寸可混淆的学习算法，并使其具有高方差。因此，高的输入维数通常需要调整分类器具有低方差和高偏置。在实践中，如果工程师能够从输入数据手动删除不相关的特征，这是有可能改善该学习功能的准确性。此外，还有许多算法的特征选择，设法确定相关特征，并丢弃不相关的。这是维数降低，其目的是将输入数据映射到较低维空间中运行的监督学习算法之前的更一般的策略的一个实例。
4、噪声中的输出值
第四个问题是在所需要的输出值（监控目标变量）的噪声的程度。如果所希望的输出值，通常是不正确的（因为人为错误或传感器的错误），则学习算法不应试图找到一个函数完全匹配的训练示例。试图以适应数据过于谨慎导致过度拟合。当没有测量误差（随机噪声），如果正在努力学习功能，是学习模式太复杂，甚至可以过度拟合。在这种情况下的目标函数，该函数不能被模拟“腐化”训练数据的那部分-这一现象被称为确定性的噪声。当任一类型的噪声存在时，最好是去一个更高的偏见，低方差估计。

## 应用
正如人们通过已知病例学习诊断技术那样，计算机要通过学习才能具有识别各种事物和现象的能力。用来进行学习的材料就是与被识别对象属于同类的有限数量样本。监督学习中在给予计算机学习样本的同时，还告诉计算各个样本所属的类别。若所给的学习样本不带有类别信息,就是无监督学习。任何一种学习都有一定的目的,对于模式识别来说，就是要通过有限数量样本的学习，使分类器在对无限多个模式进行分类时所产生的错误概率最小。
不同设计方法的分类器有不同的学习算法。对于贝叶斯分类器来说，就是用学习样本估计特征向量的类条件概率密度函数。在已知类条件概率密度函数形式的条件下，用给定的独立和随机获取的样本集，根据最大似然法或贝叶斯学习估计出类条件概率密度函数的参数。例如，假定模式的特征向量服从正态分布，样本的平均特征向量和样本协方差矩阵就是正态分布的均值向量和协方差矩阵的最大似然估计。在类条件概率密度函数的形式未知的情况下，有各种非参数方法，用学习样本对类条件概率密度函数进行估计。在分类决策规则用判别函数表示的一般情况下,可以确定一个学习目标,例如使分类器对所给样本进行分类的结果尽可能与“教师”所给的类别一致，然后用迭代优化算法求取判别函数中的参数值。
在无监督学习的情况下，用全部学习样本可以估计混合概率密度函数，若认为每一模式类的概率密度函数只有一个极大值，则可以根据混合概率密度函数的形状求出用来把各类分开的分界面。
监督学习方法是研究较为广泛的一种机器学习方法，例如神经网络传播算法、决策树学习算法等已在许多领域中得到成功的应用，但是，监督学习需要给出不同环境状态下的期望输出（即导师信号），完成的是与环境没有交互的记忆和知识重组的功能，因此限制了该方法在复杂的优化控制问题中的应用。

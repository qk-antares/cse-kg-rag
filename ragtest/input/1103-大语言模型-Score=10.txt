大语言模型（Large Language Model，简称LLM），指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。大语言模型可以处理多种自然语言任务，如文本分类、问答、对话等，是通向人工智能的重要途径。目前大语言模型采用与小模型类似的Transformer架构和预训练目标（如 Language Modeling），与小模型的区别是增加模型大小、训练数据和计算资源。
20世纪40年代末和50年代，计算机技术开始被用于研究和处理自然语言。1950年，图灵测试问世。1954年，乔治・戴沃尔设计出首台可编程机器人。1956年，达特茅斯学院举办了历史上首次人工智能研讨会，标志人工智能诞生。2020年1月23日，OpenAI发表了论文《Scaling Laws for Neural Language Models》，研究基于交叉熵损失的语言模型性能的经验尺度法则；11月30日，OpenAI公司发布ChatGPT，迅速引起社会各界关注。ChatGPT属于一类基于GPT技术的大语言模型。Google、Microsoft、NVIDIA等公司也给出了自己的大语言模型。2024年3月，马斯克的xAI公司正式发布大模型Grok-1，参数量达到3140亿，超OpenAI GPT-3.5的1750亿。
2023年12月26日，大语言模型入选“2023年度十大科技名词”。2024年4月，第27届联合国科技大会，世界数字技术院发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位多名专家学者共同编制而成。

## 发展历程

### 雏形阶段
20世纪40年代末和50年代开始采用计算机技术来研究和处理自然语言。1950年，图灵测试诞生。1954年，美国人乔治·戴沃尔设计出第一台可编程机器人。1956年，美国达特茅斯学院举行历史上第一次人工智能研讨会，标志人工智能诞生。
1966年，世界上第一个聊天机器人--ELIZA，由美国麻省理工学院（MIT）约瑟夫·魏岑鲍姆发布。ELIZA能通过脚本理解简单的自然语言，并能产生类似人类的互动。
1975年，Frederick Jelinek等人在论文《Continuous Speech Recognition by Statistical Methods》中提出并应用N-gram模型于语音识别任务。之后随着神经网络的发展，出现了神经语言模型。
1997年，长短期记忆神经网络（Long Short-Term Memory，LSTM）出现，适合处理时间序列中间隔和延迟很长的事件。
2010年，斯坦福大学推出Core NLP套件，该套件提供了一套工具和算法，帮助研究人员处理复杂的NLP任务，允许开发人员执行情感分析和命名实体识别。
2011年，出现了一个较小版本的Google Brain，具有单词嵌入等高级功能，使自然语言处理（NLP）系统能够更清楚地理解上下文。
2013年，自然语言处理模型Word2Vec诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好理解和处理文本数据。
2014年，GAN（对抗式生成网络）诞生，被誉为21世纪最强大算法模型之一，标志深度学习进入生成模型研究的新阶段。

### GPT模型问世
2017年，Google发布论文《Attention is all you need》，提出Attention机制和基于此机制的Transformer架构。此架构价值在于是一种完全基于注意力机制的序列转换模型，而不依赖RNN、CNN或者LSTM。
2018年，Google AI研究院的Jacob Devlin等人提出了BERT（Bidirectional Encoder Representation from Transformers）， BERT利用掩码机制构造基于上下文预测中间词的预训练任务，很大程度上提高自然语言处理任务的性能。BERT出现具有重大意义，尤其是预训练+参数微调”的研究范式，此后出现更多预训练语言模型都是以该范式为基础；同年，OpenAI公司同样发布了自己的模型GPT（Generative Pre-Training），这是一个典型的生成式预训练模型。
2019年，OpenAI发布GPT-2，该模型可以不用根据下游任务数据进行参数优化，可以根据给定指令自行理解并完成任务。
2020年，OpenAI发布GPT-3，并在Github上开源GPT-3部分样本和数据集。该模型拥有1750亿个参数。该模型的发布是一件跨时代的事情，意味着自然语言处理领域的大语言模型真正意义上出现了，从此正式开启大语言模型时代。

### 进阶突破阶段
2019年，Radford等人使用GPT-2模型研究大语言模型在零样本情况下的任务处理能力；Brown等人在GPT-3模型上研究通过语境学习进行少样本学习的方法指令微调将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调。
2022年，Ouyang等人提出使用“有监督微调+ 强化学习”的InstructGPT算法。
这些方法逐渐扩展到利用生成式框架针对大量任务进行有监督微调的方法，有效提升模型的性能。
2022年11月30日，OpenAI公司发布ChatGPT，该模型属于一类基于GPT技术的大语言模型。Google、Microsoft、NVIDIA等公司也给出了自己的大语言模型。
2023年，谷歌公布聊天机器人Bard，它由谷歌的大语言模型LaMDA驱动；同年，百度正式宣布将推出文心一言，3月16日正式上线。文心一言的底层技术基础为文心大模型，底层逻辑是通过百度智能云提供服务，吸引企业和机构客户使用API和基础设施，共同搭建AI模型、开发应用，实现产业AI普惠；3月，Open AI发布多模态预训练大模型GPT4.0。
2023年4月13日，亚马逊云服务部门在官方博客宣布推出Bedrock生成式人工智能服务，以及自有的大语言模型泰坦（Titan）。
2024年3月，Databricks推出大语言模型DBRX，号称“现阶段最强开源AI”；马斯克的xAI公司正式发布大模型Grok-1，参数量达到3140亿，超OpenAI GPT-3.5的1750亿；4月，在瑞士举行的第27届联合国科技大会上，世界数字技术院（WDTA）发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，是由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位的多名专家学者共同编制而成。

## 关键技术

### Transformer结构
Transformer的发展历史可以追溯到2017年，谷歌公司的研究人员在发表的论文（Attention Is All You Need）中首次介绍了Transformer，并将其应用于机器翻译任务。
Transformer是一种用于序列到序列（Sequence-to-Sequence）任务的神经网络模型，如机器翻译、语音识别和生成对话等。它是第一个完全依赖于自注意力机制来计算其输入和输出的表示的转换模型。序列到序列模型采用的是编码器-解码器结构，编码器-解码器结构采用堆叠的多头注意力机制加全连接层。通过查询-键-值的模式使用多头注意力。由于Transformer模型中既没有递归，也没有卷积，如果需要获得输入序列精准的位置信息，必须插入位置编码。位置编码和输入嵌入有相同的维度，所以二者可以实现相加运算，位置编码方式可以有多种。
Transformer结构的发展趋势：
一，更好的表征方法。未来可能会出现更好的预训练方法，更好利用大规模数据集进行模型训练。大模型可以从中受益。
二，更广泛的应用场景。目前，Transformer主要应用于自然语言处理领域，但未来可能会扩展到其他领域，如计算机视觉。
三，更好的可视化和可解释性。

### 从人类反馈中强化学习(RLHF)
RLHF是一种利用人工指导来微调预先训练好的大型语言模型（LLMs）的方法。由三个相互关联的过程组成：反馈收集、奖励建模和策略优化。RLHF优势在于能更好地与人类的意图保持一致，以及以未来的反馈为条件进行规划，从各种类型的反馈中进行流畅的学习，并根据需要对反馈进行整理。还允许机器通过抽象人类的价值学习，并不是简单地模仿人类的行为。
2023 年4月OpenAI联合创始人John Schulman在Berkeley EECS会议上所做的报告“ReinforcementLearning from Human Feedback：Progress and Challenges”，分享OpenAI在人类反馈的强化学习方面的进展，分析监督学习和强化学习各自存在的挑战。基于上述报告及相关讨论，强化学习在大语言模型上的重要作用可以概括为以下几个方面。
一，强化学习相较于有监督学习更有可能考虑整体影响。反馈粒度的不同，使强化学习更适合大语言模型，既可以兼顾表达多样性，又可以增强对微小变化的敏感性。强化学习则可以允许模型给出不同的多样性表达。
二，强化学习更容易解决幻觉问题。有监督学习算法非常容易使得求知型查询产生幻觉。在模型并不包含或者不知道答案的情况下，有监督训练仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，将放弃回答的答案赋予中低分数，将不正确的答案赋予非常高的负分，使得模型学会依赖内部知识选择放弃回答，从而在一定程度上缓解模型的幻觉问题。
三，强化学习可以更好地解决多轮对话奖励累积问题。多轮对话能力是大语言模型重要的基础能力之一。多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用有监督学习的方法构建。而使用强化学习方法，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断。

### 专家混合模型
GPT-4 采用了专家混合模型（Mixture of Experts，MoE）架构，总共有1.8 万亿个参数。GPT-4使用了16 个专家，每个专家的参数约为1110亿，每次前向传递使用2 个专家进行路由，同时还有550 亿个共享参数用于注意力机制。MoE 架构在减少推理所需的参数量的同时，仍然可以使用更大规模的模型参数。
混合专家系统类思路是目前大模型落地比较优质的路径。

### 提示学习
提示学习（Prompt-based Learning）不同于传统的监督学习，它直接利用了在大量原始文本上进行预训练的语言模型，并通过定义一个新的提示函数，使该模型能够执行小样本甚至零样本学习，以适应仅有少量标注或没有标注数据的新场景。

## 训练流程

### 预训练
预训练是大语言模型训练的首要步骤，其目标在于使模型掌握语言的统计模式与语义信息。主流的预训练阶段流程大致相同，其中关键要素是数据，需收集海量无标注数据，像互联网上的文本、新闻、博客、论坛等。这些数据可以涵盖多种语言，且要经过一定的清理和处置，去除噪声、无关信息以及涉及个人隐私的内容，最后以tokenizer粒度输入到前述的语言模型中。经清洗处理后的这些数据用于训练和优化语言模型。在预训练过程中，模型会习得词汇、句法和语义的规律以及上下文的关系。
在预训练语料集方面，GPT-3中通过主要包含经过过滤的Common Crawl数据集、WebText2、Books1、Books2以及英文Wikipedia等数据集合。其中Common Crawl的原始数据有45TB，进行过滤后仅保留了570GB的数据。通过子词方式对上述语料进行切分，大约一共包含5000亿子词。为了保证模型使用更多高质量数据进行训练，在GPT-3训练时，根据语料来源的不同，设置不同的采样权重。在完成3000亿子词训练时，英文Wikipedia的语料平均训练轮数为3.4次，而Common Crawl和Books 2仅有0.44次和0.43次。
由于Common Crawl数据集合的过滤过程繁琐复杂，OPT则采用了混合RoBERTa、Pile和Pushshift.io Redit数据的方法。由于这些数据集合中包含的绝大部分都是英文数据，因此OPT也从Common Crawl数据集中抽取了部分非英文数据加入训练语料。
BLOOM 运用 Megatron-DeepSpeed 框架进行训练，主要包括两个部分：Megatron-LM 提供张量并行能力和数据加载原语；DeepSpeed 提供 ZeRO 优化器、模型流水线以及常规的分布式训练组件。通过这种方式能够实现数据、张量和流水线的三维并行。

### 数据收集
预训练语料有两种来源：
通用语料：如网页、书籍和会话文本等，可以增强大语言模型的语言建模和泛化能力。
专业语料：有研究将预训练语料库扩展到更专业的数据集，如多语言数据、科学数据和代码，赋予大语言模型特定的任务解决能力。
数据收集完后需要对这些数据进行预处理，包括去噪、去冗余、去除不相关和潜在有毒的数据。

### 基础大模型训练
由于模型参数量和所使用的数据量巨大。所以普通服务器单机无法完成训练过程，因此通常采用分布式架构完成训练。

### 指令微调
通过指令微调，大模型学习到了如何响应人类指令，可以根据指令直接能够生成合理的答案。
在完成预训练后，就可以通过指令微调去挖掘和增强语言模型本身具备的能力，这步也是很多企业以及科研研究人员利用大模型的重要步骤。
Instruction tuning（指令微调）是大模型训练的一个阶段，它是一种有监督微调的特殊形式，旨在让模型理解和遵循人类指令。在指令微调阶段，首先需要准备一系列的NLP任务，并将每个任务转化为指令形式，其中指令包括人类对模型应该执行的任务描述和期望的输出结果。然后，使用这些指令对已经预训练好的大语言模型进行监督学习，使得模型通过学习和适应指令来提高其在特定任务上的表现。
为了让模型训练更加高效和简单，这个阶段还有一种高效的fine-tuning技术，这为普通的从业者打开了通向使用大模型的捷径。
Parameter-Efficient Fine-Tuning（PEFT）旨在通过最小化微调参数的数量和计算复杂度，达到高效的迁移学习的目的，提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。在训练过程中，预训练模型的参数保持不变，只需微调少量的额外参数，就可以达到与全量微调相当的性能。
目前，很多研究对PEFT方法进行了探索，例如Adapter Tuning和Prefix Tuning等。其中，Adapter Tuning方法在面对特定的下游任务时，将预训练模型中的某些层固定，只微调接近下游任务的几层参数。而Prefix Tuning方法则是在预训练模型的基础上，添加一些额外的参数，这些参数在训练过程中会根据特定的任务进行更新和调整。
工业界现在常用的Adapter Tuning的技术是Low-Rank Adaptation（LoRA） 。它通过最小化微调参数的数量和计算复杂度，实现高效的迁移学习，以提高预训练模型在新任务上的性能。LoRA 的核心思想是将预训练模型的权重矩阵分解为两个低秩矩阵的乘积。通过这种分解，可以显著减少微调参数的数量，并降低计算复杂度。该方式和机器学习中经典的降维的思想很类似，类似地，LoRA 使用了矩阵分解技术中的奇异值分解 (Singular Value Decomposition, SVD) 或低秩近似 (Low-Rank Approximation) 方法，将原始权重矩阵分解为两个低秩矩阵的乘积。
在微调过程中，LoRA 只更新这两个低秩矩阵的参数，而保持其他预训练参数固定不变。这样可以显著减少微调所需的计算资源和时间，并且在很多任务上取得了与全量微调相当的性能。
LoRA技术的引入使得在大规模预训练模型上进行微调更加高效和可行，为实际应用提供了更多可能性。

### 类人对齐
由于模型输出的结果与人类回答差距很大，因此需要进一步优化模型，使模型的输出与人类习惯对齐。其中OpenAI开发ChatGPT的人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF）是最具代表性也是最成功的。

### 奖励建模
奖励建模（Reward Modeling）阶段的目标是构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。奖励模型（RM模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM模型与基础语言模型和SFT模型不同，RM模型本身并不能单独提供给用户使用。
奖励模型的训练通常和SFT模型一样，使用数十块GPU，通过几天时间完成训练。由于RM模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训练数据对该模型进行训练

### 强化学习
强化学习（Reinforcement Learning）阶段根据数十万用户给出的提示词，利用前一阶段训练的RM模型，给出SFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。
使用强化学习，在SFT模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段需要的计算量相较预训练阶段也少很多，通常仅需要数十块GPU，数天即可完成训练。
Andrej Karpathy也指出，强化学习并不是没有问题的，它会使基础模型的熵降低，从而减少了模型输出的多样性。经过强化学习方法训练后的RL模型，就是最终提供给用户使用、具有理解用户指令和上下文的类ChatGPT 系统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM模型的准确率问题，使得在大语言模型上有效应用强化学习非常困难。

## 工作原理
大语言模型的原理是基于深度学习，它利用大量的数据和计算资源来训练具有大量参数的神经网络模型。通过不断地调整模型参数，使得模型能够在各种任务中取得最佳表现。通常说的大模型的“大”的特点体现在：参数数量庞大、训练数据量大、计算资源需求高等。很多先进的模型由于拥有很“大”的特点，使得模型参数越来越多，泛化性能越来越好，在各种专门的领域输出结果也越来越准确。现在市面上比较流行的任务有AI生成语言（ChatGPT类产品）、AI生成图片（Midjourney类产品）等，都是围绕生成这个概念来展开应用。“生成”简单来说就是根据给定内容，预测和输出接下来对应内容的能力。

## 训练成本
训练通用大模型非常“烧钱”。国盛证券报告《ChatGPT需要多少算力》估算，GPT-3训练一次的成本约为140万美元。一些更大规模的大型语言模型训练成本更高，处于200万美元-1200万美元区间。以ChatGPT在1月的独立访客平均数1300万来计算，其对应的芯片需求为 3万多片英伟达A100 GPU，初始投入成本约8亿美元，每日电费约5万美元。若将当前的ChatGPT部署到谷歌的每次搜索中，需要512820.51台A100 HGX服务器以及总共4102568个A100 GPU，这些服务器和网络仅资本支出就超过1000亿美元。

## 大语言模型对比
发布时间
模型名称
发布机构
所在国家
模型参数量（亿）
模态
最大序列长度
使用方式
2024年7月18日
GPT-4o mini
OpenAI
美国
-
文本、图像、音频、视频
-
-
2024年5月14日
GPT-4o
OpenAI
美国
-
文本、音频、图像
-
-
2023年3月
GPT-4
OpenAI
美国
未知
语言、图像
32K
API
2023年3月
Claude
Anthropic
美国
未知
语言
100K
受限访问
2023年2月
LLaMA
Meta
美国
650
语言
2K
开源
2023年2月
MOSS
复旦大学
中国
160
语言
2K
开源
2023年1月
Anthropic-LM
Anthropic
美国
520
语言
8K
未知
2022年11月
ChatGPT
OpenAI
美国
未知
未知
未知
未知
2022年7月
BLOOM
BigScience
法国
1760
语言
2K
开源
2022年5月
OPT
Meta
美国
1750
语言
2K
开源
2022年4月
PaLM
Google
美国
5400
语言
2K
API
-
CPM-2
清华、智源
中国
1980
语言
未知
开源
2021年8月
Codex
OpenAI
美国
120
代码
未知
API
2021年4月
盘古-α
华为
中国
2000
语言
1K
未知
2020年5月
GPT-3
OpenAI
美国
1750
语言
2K
API
2020年2月
Turing-NLG
Microsoft
美国
170
语言
未知
未知
2019年10月
T5
Google
美国
110
语言
512
开源
2016年1月
CodeGen
Salesforce
美国
160
代码
2K
开源
-
CPM-2
清华、智源
中国
1980
语言
未知
开源
-
J1-Jumbo
AI21 Labs
美国
1780
语言
2K
受限访问
-
GPT-NeoX
EleutherAI
未知
200
语言
2K
开源
-
M6-10T
阿里巴巴
中国
100000
语言、图像
512
未知
-
YaLM
Yandex
俄罗斯
1000亿
语言
2K
开源
参考资料

## 应用场景

### 教育领域
在线讨论与反思学习场景：赋能高阶思维能力培养
在线讨论与反思学习场景中的文本数据在一定程度上反映学生在线学习过程中的认知和情感表现。具有自然语言理解优势的BERT可对学生文本数据中的认知与情感进行识别，为赋能学生高阶思维能力培养奠定基础。同时探究学生在线学习认知和情感发展规律。
人机协同提问场景：加强阅读理解能力
自我提问可以促进学习专注度，加深对阅读内容的理解，但当前学生提问普遍存在水平不高、类型单一等问题。对此，可以利用T5和GPT系列的自然语言生成优势，为高质量问题创建提供支持，进而加强学生的阅读理解能力。利用GPT-3自动生成提示语（包括提问类型、答案、提问视角），通过多轮人机对话，帮助学生提出深层次问题。GPT-3更能促使小学生提出一系列与知识点相关的、深层次的问题，以加强深度阅读理解。总的来说，大语言模型可以利用其文本生成优势，通过人机协同对话形式辅助学生提问，进而提升其阅读理解能力。
人机协同写作和数学解题场景：提升写作和解题水平
写作与数学解题逻辑教学作为学科教学领域的两项重难点，一直存在学生写作时“不愿写”“没得写”“不会写”和数学解题答题不规范、传统教学指导效率低等问题。对此，GPT系列或类T5结构模型因其内容创作和数学推理优势，可以广泛应用于智能写作工具研究和数学解题辅助研究领域，进而有效提升学生的写作和数学解题水平。

### 金融业
金融行业需要处理海量文本信息，大语言模型有助于分析和提取新闻媒体、研究报告、财务报表、企业公告、政府政策等文本信息中的价值。同时，金融信息具有强时效性，大语言模型可以做出秒级分析并提出建议。对于负债业务，基于大语言模型的智能客服可以协助优化存款业务流程，同时节省人力成本，提升服务效率。

### 办公软件
2024世界人工智能大会上，金山办公发布WPS AI 2.0，并推出政务自研模型——金山政务办公模型1.0。WPS AI是金山办公旗下基于大语言模型的人工智能办公助手。WPS AI演示了升级后为个人用户新增的4个AI办公助手，分别是AI写作助手、AI阅读助手、AI数据助手、AI设计助手。
快手在大会期间正式推出视频生成大模型可灵网页端。同时，可灵推出更加清晰的高画质版、首尾帧控制、镜头控制等新功能，创作者单次生成的文生视频时长增加至10秒。

### 客户联络领域
提升自动回复能力
可以根据用户输入的问题提供快速和准确的响应，快速解决问题，节省客服团队大量的时间和资源，提高客户体验和满意度。
强化意图识别能力
观察客户联络领域所处现状，大部分是把简单、重复、流程性的问题，交给机器人处理；复杂的、需要情感关怀的问题，交由人工客服处理。而传统的智能客服在意图理解方面的能力，仍然相对薄弱。借助大模型，智能客服能够有效结合用户的历史对话、当前沟通内容等上下文语境，更精准地识别出用户的需求和意图。
优化人机交互体验
以ChatGPT为例来看，大模型的深度应用开创了客户使用体验的新范本。丰富的参数和强大的内容生成能力，能够支持智能客服实现更加个性化的问答回复，而非过往千篇一律的机械式问答。
丰富实际应用场景
ChatGPT的应用目前已经有相对确定的场景，如扮演人工客服与客户沟通专业知识、提供专业的问答知识建议、对沟通记录进行质检标记、主动分析座席工作行为、发起产品推介、闲聊寒暄以及更“人性化”的引导留资等。

## 局限性
不能创造语言
大模型至多是会使用语言，而远谈不上能创造语言、发明语言。大语言模型的基础仍然是深度学习技术，即利用大量的文本数据来训练模型，只不过模型的参数规模更为庞大，但与产生语言的劳动、实践根本不沾边。
不能深度理解人类
大语言模型目前只是人类生存实践的旁观者和应答者，缺乏共情能力，还达不到像人类理解那样的深刻性与丰富性，而深层理解更彰显人类智能的特殊性。
不能全面嵌入社会
以ChatGPT为代表的大语言模型仍然不能像人一样在社会中进行交往与实践，不能以人类体悟语境的方式来体悟语境，因此，谈论ChatGPT拥有媲美人类的智能，完全理解人类的语言，还为时尚早。
安全性不高
安全性堪称大型语言模型必须直面的关键问题之一。大型语言模型可以在众多学科领域的任务中得以应用，然而，这也表明此类模型会遭遇广泛的内容安全难题。尽管大型语言模型已借助基于人类反馈的强化学习等诸多方式，努力使模型输出与人类价值观相契合，但在应用于各个领域时，语言模型依旧容易遭到恶意利用，进而生成诸如偏见言论、煽动性话语、隐私侵犯言论等存在安全隐患的文本。
成本高昂
大语言模型在训练和部署过程中，会耗费大量的计算资源与人力资源，成本高昂。对部分中小型企业来说，很难承受这样的成本，也难以获取充足的技术支持及资源。在企业级应用方面，采用百亿级基础模型较为适宜，再依据不同需求去训练相应的垂直模型，如此只需承担垂直训练的成本。不过，企业怎样实现高效的垂直训练以及如何把控成本，依旧是大模型需要面对的问题之一。
不能保障内容可信
可信度当前是大型语言模型的重大局限之一。虽然大语言模型能够用于处理各种真实场景中的问题，然而它依旧会产出不可信的文本。现今使用者只能按照自身需求去核验生成的内容是否真实可靠，很难具备权威说服力。与此同时，模型在解决涉及推理的问题时，有可能由于推理过程出现错误而得到不可信的结果。这对其研究发展以及应用落地都有着负面的影响。

## 社会影响

### 年度词汇
2023年12月6日，大语言模型入选国家语言资源监测与研究中心发布的“2023年度中国媒体十大流行语”。
2023年12月26日，大语言模型入选“2023年度十大科技名词”。

### 科技发展
大语言模型的快速进步，正在激发新业态、新模式，由此带来的工作方式、教育模式等的变革。它不仅是一项技术，更是未来国力竞争与生产力提高的重要资源。以深度学习平台和大模型为代表的AI新型基础设施，对科技创新、产业升级和高质量发展意义重大。
